{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import codecs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \n",
    "    with open(path, encoding='utf-8', mode='r+') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    return data.strip().split('\\n')\n",
    "\n",
    "def preprocess_word(word):\n",
    "    \n",
    "    s = re.sub(r'([.,?!])', r' \\1', word)\n",
    "    s = re.sub(r'[^a-zA-Z.\\'!?]+', r' ', word)\n",
    "\n",
    "    return s\n",
    "\n",
    "def split_en_fra(line):\n",
    "    \n",
    "    en = []\n",
    "    fra = []\n",
    "    pairs = []\n",
    "    \n",
    "    for l in line:\n",
    "        pair = l.lower().split('\\t')\n",
    "        en_word = pair[0]\n",
    "        fra_word = pair[1]\n",
    "\n",
    "        en.append(preprocess_word(pair[0]))\n",
    "        fra.append(preprocess_word(pair[1]))\n",
    "        pairs.append(pair)\n",
    "        \n",
    "    return en, fra, pairs\n",
    "\n",
    "def filter_pairs(pairs, max_length=10):\n",
    "    \n",
    "    filtered_pairs = []\n",
    "    \n",
    "    eng_prefixes = (\n",
    "        \"i am\", \"i m \",\n",
    "        \"he is\", \"he s\",\n",
    "        \"she is\", \"she s\",\n",
    "        \"you are\", \"you re\",\n",
    "        \"we are\", \"we re\",\n",
    "        \"they are\", \"they re\"\n",
    "    )\n",
    "    \n",
    "    for p in pairs:\n",
    "        pair1 = preprocess_word(p[0])\n",
    "        pair2 = preprocess_word(p[1])\n",
    "        if len(pair1.split(' ')) < max_length and len(pair2.split(' ')) < max_length and pair1.startswith(eng_prefixes):\n",
    "            filtered_pairs.append([pair1, pair2])\n",
    "            \n",
    "    return filtered_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = read_file('./data/eng-fra.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tVa !',\n",
       " 'Run!\\tCours\\u202f!',\n",
       " 'Run!\\tCourez\\u202f!',\n",
       " 'Wow!\\tÇa alors\\u202f!',\n",
       " 'Fire!\\tAu feu !']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, fra, pairs = split_en_fra(line)\n",
    "filtered_pairs = filter_pairs(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go.', 'run!', 'run!', 'wow!', 'fire!']\n",
      "['va !', 'cours !', 'courez !', ' a alors !', 'au feu !']\n",
      "[['go.', 'va !'], ['run!', 'cours\\u202f!'], ['run!', 'courez\\u202f!'], ['wow!', 'ça alors\\u202f!'], ['fire!', 'au feu !']]\n",
      "[['i am fat.', 'je suis gras.'], ['he is ill.', 'il est malade.'], ['he is old.', 'il est vieux.'], ['i am busy.', 'je suis occup .'], ['i am calm.', 'je suis calme.']]\n",
      "135842\n",
      "3775\n"
     ]
    }
   ],
   "source": [
    "print(en[:5])\n",
    "print(fra[:5])\n",
    "print(pairs[:5])\n",
    "print(filtered_pairs[:5])\n",
    "print(len(pairs))\n",
    "print(len(filtered_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(eng_list, fra_list):\n",
    "    \n",
    "    start_token = 0\n",
    "    end_token = 1\n",
    "    \n",
    "    en_idx = {}\n",
    "    fra_idx = {}\n",
    "    idx_en = {}\n",
    "    idx_fra = {}\n",
    "    \n",
    "    en_idx['SOS'] = start_token\n",
    "    en_idx['EOS'] = end_token\n",
    "    fra_idx['SOS'] = start_token\n",
    "    fra_idx['EOS'] = end_token\n",
    "    \n",
    "    idx = 2\n",
    "    for e in eng_list:\n",
    "        for word in e.split():\n",
    "            if word not in en_idx:\n",
    "                en_idx[word] = idx\n",
    "                idx += 1\n",
    "    idx = 2\n",
    "    for f in fra_list:\n",
    "        for word in f.split():\n",
    "            if word not in fra_idx:\n",
    "                fra_idx[word] = idx\n",
    "                idx += 1\n",
    "            \n",
    "    idx_en[0] = 'SOS'\n",
    "    idx_en[1] = 'EOS'\n",
    "    idx_fra[0] = 'SOS'\n",
    "    idx_fra[1] = 'EOS'\n",
    "        \n",
    "    for word, idx in en_idx.items():\n",
    "        idx_en[idx] = word\n",
    "    for word, idx in fra_idx.items():\n",
    "        idx_fra[idx] = word\n",
    "    \n",
    "    return en_idx, fra_idx, idx_en, idx_fra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_idx, fra_idx, idx_en, idx_fra = build_dict(en, fra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_idx) == len(idx_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_to_idx(sentence):\n",
    "    return [en_idx[word] for word in sentence.split(' ')]\n",
    "\n",
    "def fra_to_idx(sentence):\n",
    "    return [fra_idx[word] for word in sentence.split(' ')]\n",
    "\n",
    "def to_tensor(indexes):\n",
    "    indexes.append(1)\n",
    "    return torch.tensor(indexes, device=device).view(-1, 1)\n",
    "\n",
    "def tensor_from_pairs(pair):\n",
    "    input_tensor = to_tensor(en_to_idx(pair[0].strip()))\n",
    "    target_tensor = to_tensor(fra_to_idx(pair[1].strip()))\n",
    "    \n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he is a heroin addict.', 'il est h ro no d pendant.']\n",
      "(tensor([[  66],\n",
      "        [ 245],\n",
      "        [ 110],\n",
      "        [7011],\n",
      "        [2851],\n",
      "        [   1]], device='cuda:0'), tensor([[  71],\n",
      "        [  73],\n",
      "        [ 880],\n",
      "        [1805],\n",
      "        [3707],\n",
      "        [  97],\n",
      "        [7183],\n",
      "        [   1]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "rand_idx = random.randint(0, len(filtered_pairs))\n",
    "print(filtered_pairs[rand_idx])\n",
    "print(tensor_from_pairs(filtered_pairs[rand_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_tensor = []\n",
    "# fra_tensor = []\n",
    "\n",
    "# for e in en:\n",
    "#     en_tensor.append(to_tensor(en_to_idx(e.strip())))\n",
    "# for f in fra:\n",
    "#     fra_tensor.append(to_tensor(fra_to_idx(f.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        embed = self.embedding(inp).view(1, 1, -1)\n",
    "        output = embed\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Dense(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        output = self.embedding(inp).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.ouut(output[0]))\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=10):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, inp, hidden, encoder_outputs):\n",
    "        embed = self.embedding(inp).view(1, 1, -1)\n",
    "        embed = self.dropout(embed)\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embed[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embed[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_opt, decoder_opt, crit, max_length=10):\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    \n",
    "    encoder_opt.zero_grad()\n",
    "    decoder_opt.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    training_loss = 0\n",
    "    for i in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "        encoder_outputs[i] = encoder_output[0,0]\n",
    "        \n",
    "    decoder_input = torch.tensor([[0]], device=device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            training_loss += crit(decoder_output, target_tensor[i])\n",
    "            decoder_input = target_tensor[i]\n",
    "            \n",
    "    else:\n",
    "        for i in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            training_loss += crit(decoder_output, target_tensor[i])\n",
    "            \n",
    "            if decoder_input.item() == 1:\n",
    "                break\n",
    "                \n",
    "    training_loss.backward()\n",
    "    encoder_opt.step()\n",
    "    decoder_opt.step()\n",
    "    \n",
    "    return training_loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iters(encoder, decoder, n_iters, print_every=10, plot_every=100, lr=0.01):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_opt = optim.SGD(encoder.parameters(), lr=lr)\n",
    "    decoder_opt = optim.SGD(decoder.parameters(), lr=lr)\n",
    "#     training_pairs = [tensor_from_pairs(random.choice(filtered_pairs)) for _ in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        rand_pairs = random.choice(filtered_pairs)\n",
    "        training_pairs = tensor_from_pairs(rand_pairs)\n",
    "#         training_pair = training_pairs[i]\n",
    "        input_tensor = training_pairs[0]\n",
    "        target_tensor = training_pairs[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_opt, decoder_opt, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            train_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\"Loss:\", train_loss_avg)\n",
    "            \n",
    "#         if i % plot_every == 0:\n",
    "#             plot_loss_avg = print_loss_total / plot_every\n",
    "#             plot_losses.append(plot_loss_avg)\n",
    "#             plot_loss_avg = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  66],\n",
       "         [ 144],\n",
       "         [2012],\n",
       "         [   1]], device='cuda:0'),\n",
       " tensor([[ 71],\n",
       "         [  6],\n",
       "         [299],\n",
       "         [284],\n",
       "         [  1]], device='cuda:0'))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_from_pairs(random.choice(filtered_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0020491358947753906\n",
      "Loss: 4.4502176980467025\n",
      "Loss: 3.557711275358076\n",
      "Loss: 2.9388878632730724\n",
      "Loss: 2.400896590808083\n",
      "Loss: 1.9453321599769453\n",
      "Loss: 1.5924743188971298\n",
      "Loss: 1.275214881206147\n",
      "Loss: 1.0841896553254342\n",
      "Loss: 0.8951137886225994\n",
      "Loss: 0.7469192901023616\n",
      "Loss: 0.6370441047694261\n",
      "Loss: 0.5743444136838698\n",
      "Loss: 0.527690010658919\n",
      "Loss: 0.4842822133519212\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(len(en_idx), hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, len(fra_idx), dropout_p=0.1).to(device)\n",
    "\n",
    "train_iters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=10):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = to_tensor(en_to_idx(sentence))\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        \n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "            encoder_outputs[i] += encoder_output[0,0]\n",
    "        \n",
    "        decoder_input = torch.tensor([[0]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[i] = decoder_attention.data\n",
    "            \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == 1:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(idx_fra[topi.item()])\n",
    "                \n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "        return decoded_words, decoder_attention[:i + 1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(filtered_pairs)\n",
    "        print('English:', pair[0])\n",
    "        print('Actual French translation:', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('Model translation:', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: he sometimes watches tv.\n",
      "Actual French translation: il regarde parfois la t l .\n",
      "Model translation: il regarde parfois la t l . <EOS>\n",
      "\n",
      "English: you really don't have a clue do you?\n",
      "Actual French translation: t'as vraiment pas l'ombre d'une id e si ?\n",
      "Model translation: vous tes vraiment ignare non ? <EOS>\n",
      "\n",
      "English: i am on holiday this week.\n",
      "Actual French translation: je suis en cong cette semaine.\n",
      "Model translation: je suis en cong cette semaine. cette semaine. <EOS>\n",
      "\n",
      "English: he is unpopular for some reason.\n",
      "Actual French translation: il est impopulaire pour une raison quelconque.\n",
      "Model translation: il est impopulaire pour une raison quelconque. <EOS>\n",
      "\n",
      "English: he is a bank clerk.\n",
      "Actual French translation: il est employ de banque.\n",
      "Model translation: il est employ de banque. <EOS>\n",
      "\n",
      "English: he is having lunch now.\n",
      "Actual French translation: il est en train de d jeuner l'heure actuelle.\n",
      "Model translation: il est en train de d jeuner l'heure actuelle. <EOS>\n",
      "\n",
      "English: i am yours and you are mine.\n",
      "Actual French translation: je suis tien et tu es mien.\n",
      "Model translation: je suis vous et vous tes moi. <EOS>\n",
      "\n",
      "English: she is never late for school.\n",
      "Actual French translation: elle n'est jamais en retard l' cole.\n",
      "Model translation: elle n'est jamais en retard l' cole. <EOS>\n",
      "\n",
      "English: i am the leader of this team.\n",
      "Actual French translation: je suis le chef de cette quipe.\n",
      "Model translation: je suis le chef de cette quipe. <EOS>\n",
      "\n",
      "English: i am the leader of this team.\n",
      "Actual French translation: je suis le chef de cette quipe.\n",
      "Model translation: je suis le chef de cette quipe. <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
